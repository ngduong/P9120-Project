---
title: "Homework 4"
author: "Ngoc Duong"
date: "11/26/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(e1071)
library(ModelMetrics)
library(caret)
library(microbenchmark)
library(broom)
library(tree)
library(ISLR)
library(ggfortify)
```

## 8.4. Problem 5

Given a two-class classification problem (Red and Green), ten bootstrapped samples are obtained from the data. The estimates of $P(Y = Red | X)$ were obtained from a classification tree on these bootstrapped samples as: 0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75.

Under the two approaches to combine the estimated probabilities into a single class prediction:

```{r}
# Q8.4.5
p = c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
pred.red = mean(p >= 0.5) #majority vote approach
pred.red
phat = mean(p) #mean probability approach
phat
```

* Majority vote approach: there are 6 out of 10 trees that predicted class red as outcome given X, or 60%, so using this approach the final single class prediction is "red".

* Average probability approach: taking the average of these 10 probabilities gives (0.1 + 0.15 + 0.2 + 0.2 + 0.55 + 0.6 + 0.6 + 0.65 +  0.7 + 0.75)/10 = 0.45 < 0.5; therefore, the final classification under this approach is "green". 

## 8.4. Problem 9

**a) Create training set -- random sample of 800, and test set with remaining observations**

```{r echo = FALSE, message = FALSE}
data(OJ)
#create training set with 800 obs and test set with remaining obs
oj_data = OJ %>% janitor::clean_names() %>% 
  mutate(purchase = as.factor(purchase))

set.seed(1)
rowTrain = sample(nrow(oj_data), 800)
oj_train = oj_data[rowTrain,]
oj_test = oj_data[-rowTrain,]
```

**b) Fit a tree to the data and describe results**

```{r echo  =FALSE, message = FALSE}
#b) fit a tree 
tree.mod = tree(purchase ~., data = oj_train)
summary(tree.mod)
```

From the summary output, we see the tree has 9 terminal nodes, and the splitting criteria are from these 5 variables: "loyal_ch", "price_diff", "special_ch", list_price_diff", and "pct_disc_mm". The training error rate is 127/800, or 15.88%. 

**c) Detailed text output and interpretation**

```{r echo = FALSE, message = FALSE}
#c) detailed text output
tree.mod
```

The nodes with asterisk signs denote terminal nodes. We can randomly pick a terminal node (9). This node gives the split criterion (loyal_ch > 0.03564), but we also need to incorporate the parent nodes' splitting criteria (loyal_ch < 0.5036 and loyal_ch < 0.2808), so combined together, we have the overall predicted class "MM" for "loyal_ch" between 0.0356 and 0.2754. 

The number of observations falling into this branch is 118, and 19.42% (or 23 observations) of those have class "CH" while 80.51% have class "MM" (or 95 observations). The deviance (which measures node impurity for classification tree) is 116.4 for this branch.

**d) Tree plot and interpretation**

```{r echo = FALSE, message = FALSE}
#d) create plot for tree
plot(tree.mod)
text(tree.mod, pretty = 0, cex = 0.75)
```

The most important predictor of "purchase" seems to be "loyal_ch", since the first split differentiates "CH" (loyal_ch > 0.7645) from "MM" (loyal_ch < 0.2808) using this criterion.

For regions of loyal_ch between 0.2808 and 0.7645, "price_diff" and "list_price_diff" determines the split points for the next branches, followed by special_ch and "pct_disc_mm" which return the terminal nodes. Overall, it seems like observations with lower "loyal_ch", "price_diff" and "special_ch" are more likely to be classified as "MM".

**e) Predict reponse on test data and produce confusion matrix**

```{r echo = FALSE, message = FALSE}
tree.pred = predict(tree.mod, oj_test[,-1], type = "class") #predict on test data
table(tree.pred, oj_test$purchase) #confusion matrix
#misclassification rate
(table(tree.pred, oj_test$purchase)[1,2] + table(tree.pred, oj_test$purchase)[2,1])/270
```

Applying the classification tree obtained from training data to test data, we have the test error rate as (`r table(tree.pred, oj_test$purchase)[1,2]` + `r table(tree.pred, oj_test$purchase)[2,1]`)/270 = `r (table(tree.pred, oj_test$purchase)[1,2] + table(tree.pred, oj_test$purchase)[2,1])/270`.

**f) Apply cv.tree function**

We use the argument FUN = prune.misclass to specify misclassification rate as the criterion to guide the cross-validation and pruning process. 

```{r}
set.seed(77)
cv.tree.mod = cv.tree(tree.mod, FUN=prune.misclass, K = 10)
```

**g) Plot CV error against tree sizes**

```{r echo = FALSE, message = FALSE}
#plot CV error vs. tree size
plot(cv.tree.mod$size, cv.tree.mod$dev/nrow(oj_train), 
     type = "b", xlab = "Tree size", ylab = "CV misclassification rate")
#cv.tree.mod$size
#cv.tree.mod$dev
```

**h) Optimal tree size (CV)**

```{r message = FALSE}
#size of tree with minimum error
cv.tree.mod$size[which.min(cv.tree.mod$dev)]
```

From the plot, we can see tree size 7 corresponds to the lowest CV-classification error rate in this case. This is optimal tree size is specific to this particular training data which also depends on the set seed.

**i) Pruned tree**

```{r echo=FALSE, message = FALSE, warning = FALSE}
pruned.tree.mod = prune.tree(tree.mod, best = 7)
summary(pruned.tree.mod)
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
plot(pruned.tree.mod)
text(pruned.tree.mod, pretty = 0, cex = 0.75)
```

**j) Compare training error rate between pruned and unpruned tree**

On the training set, the pruned tree gives slightly higher training misclassification rate than the unpruned tree (16.25% vs. 15.88%). This is reasonable, as the unpruned tree fits the training data very well, but may cause overfitting/high variance when applied to test set.

**k) Compare test error rate between pruned and unpruned tree**

```{r echo = FALSE, message = FALSE}
prune.tree.pred = predict(pruned.tree.mod, oj_test[,-1], type = "class")
cm.prune = table(prune.tree.pred, oj_test$purchase)
cm.prune

(cm.prune[1,2]+cm.prune[2,1])/270
```

The test error rate for the pruned tree is (`r cm.prune[1,2]` + `r cm.prune[2,1]`)/270 = `r round((cm.prune[1,2]+cm.prune[2,1])/270,3)` which is lower than the test error rate for the unpruned tree (0.1704). Pruned trees might have lower variance compared to unpruned trees.

## 10.7. Problem 11

**a) Load in the dataset. The data consist 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.**

```{r}
# Q10.7.11
data = read.csv("ch10ex11.csv", header = FALSE)

data_label = as.data.frame(t(data)) #transpose
data_label$status = c(rep("Healthy", 20), rep("Diseased", 20)) #add disease status label
```

**b) Apply hierarchical clustering using correlation-based distance, and plot the dendogram.**

```{r echo = FALSE, message = FALSE, fig.align='center'}
#hierarchical clustering
cor.dist <-  as.dist(1 - cor(scale(data))) #correlation-based distance

#specify clustering types
hc.complete <-hclust(cor.dist, method = "complete")
hc.average <-hclust(cor.dist, method = "average")
hc.single <-hclust(cor.dist, method = "single")

#plot dendograms
plot(hc.complete, labels = colnames(data), main = "Complete linkage", xlab = "", ylab = "", sub = "")
plot(hc.average, labels = colnames(data), main = "Average Linkage", xlab = "", ylab = "", sub = "")
plot(hc.single, labels = colnames(data), main = "Single Linkage", xlab = "", ylab = "", sub = "")
```

**Do the genes separate the samples into two groups? Do results depend on linkage type?**

The genes generally separate the samples into two groups (for complete linkage and single linkage) using correlation-based distance. The two groups are a little less distinctive when using "average linkage" (there can be 3 or 4 groups depending on where to cut the tree/dendogram). Since we end up with different clusterings, the results depend on the type of linkage used. 

**Which genes differ the most across the two groups?**

To look at which genes differ the most across the two types of patients, we can first check to see if PC's from PCA separate the two classes well. Then, we can look at the loading vectors from PCA to see which genes can explain the variance the most.

```{r echo = FALSE, message = FALSE}
#PCA
pca = prcomp(data_label[,-1001])
```

We can look at the variations captured by the PCs individually and cumulatively

```{r echo = FALSE, message = FALSE}
summary(pca) #variation captured by 40PCs
autoplot(pca, data = data_label, colour = "status", 
         frame = TRUE, frame.type = "norm") #clustering of subjects based on first 2PCs
```


```{r echo = FALSE, message = FALSE}
load.total = apply(pca$rotation, 1, sum) #obtain loadings for each gene
index = order(abs(load.total), decreasing = TRUE) #order the genes with highest loading first
```

* After getting the loadings for each gene, the first 20 genes with highest loadings and their loadings maginitude are as follows

```{r echo = FALSE, message = FALSE}
index[1:20] #first 20 genes with highest loadings
abs(load.total[index[1:20]]) #the loadings for these 20 genes
```

From the plot, we can see that the first 2 PCs separate the classes well, and by obtaining the loading vectors for the genes, we can rank the loadings to get the top 20 differentiating genes (genes 865, 68, 911, 428, 624, 11, etc.)

## Appendix 

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```


